{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지식 증류 함수\n",
    "def distillation_loss(student_output, teacher_output, true_labels, alpha):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    distillation_loss = loss_fn(student_output, teacher_output)\n",
    "    student_loss = loss_fn(student_output, true_labels)\n",
    "    return alpha * distillation_loss + (1 - alpha) * student_loss\n",
    "\n",
    "# 학생 모델 학습\n",
    "class DistilledLSTMModel:\n",
    "    def __init__(self, teacher_model, input_size, hidden_size, num_layers, output_size, learning_rate, gradient_threshold, epoch, alpha):\n",
    "        self.teacher_model = teacher_model\n",
    "        self.model = _LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.1)\n",
    "        self.epoch = epoch\n",
    "        self.gradient_threshold = gradient_threshold\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        print('Distilled LSTM Training')\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(-1)\n",
    "        \n",
    "        self.teacher_model.eval()\n",
    "        self.model.train()\n",
    "        for epoch in tqdm(range(self.epoch)):\n",
    "            student_outputs = self.model(X_train_tensor)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = self.teacher_model(X_train_tensor)\n",
    "            \n",
    "            loss = distillation_loss(student_outputs, teacher_outputs, y_train_tensor, self.alpha)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_threshold)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{self.epoch}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "            # self.scheduler.step()\n",
    "        \n",
    "        print('Distilled LSTM Training Done')\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(X_tensor).detach().numpy()\n",
    "        return y_pred\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.model.state_dict(), f'{path}.pth')\n",
    "        print(\"학생 모델 상태 저장 완료\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
